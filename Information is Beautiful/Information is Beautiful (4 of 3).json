{"paragraphs":[{"text":"%pyspark\nimport sys\nprint(sc.version)\nprint(sys.version)","dateUpdated":"2017-04-01T13:06:14+0200","config":{"editorMode":"ace/mode/python","colWidth":12,"results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774270_219030781","id":"20170321-102536_1353198708","dateCreated":"2017-04-01T13:06:14+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9027"},{"title":"Initialize ZeppelinSession ...","text":"%pyspark\n\nfrom zeppelin_session import ZeppelinSession, resetZeppelinSession, LogLevel, Logger\n\nresetZeppelinSession(z.z)\n\nzs = ZeppelinSession(z.z)\n","user":"anonymous","dateUpdated":"2017-04-01T13:06:28+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774271_218646032","id":"20170320-190838_55365813","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:06:28+0200","dateFinished":"2017-04-01T13:06:28+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9028"},{"title":"... and start it in the next paragraph","text":"%pyspark\n\nzs.start()","user":"anonymous","dateUpdated":"2017-04-01T13:06:31+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774272_302906041","id":"20170320-190902_1117826594","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:06:31+0200","dateFinished":"2017-04-01T13:06:31+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9029"},{"text":"%pyspark\n\nclass Breaker:\n    def __init__(self, zeppelinContext):\n        self.z = zeppelinContext\n        print(\"\"\"%angular\\n<button id=\"__breaker__\" ng-click=\"__stop__ = 1\">Stop</button>\"\"\")\n    \n    def start(self):\n         self.z.angularBind(\"__stop__\", 0)\n        \n    def isStopped(self):\n        return self.z.angular(\"__stop__\") == 1\n\n","user":"anonymous","dateUpdated":"2017-04-01T13:06:33+0200","config":{"editorMode":"ace/mode/python","colWidth":12,"results":[],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774272_302906041","id":"20170320-211052_28428655","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:06:33+0200","dateFinished":"2017-04-01T13:06:33+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9030"},{"title":"Initialize NVD3","text":"%pyspark\n\nfrom nvd3_stat import Nvd3\n\nnv = Nvd3()\n\nnv.reloadNVD3(\"1.8.5\")\n","user":"anonymous","dateUpdated":"2017-04-01T13:06:54+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774273_302521292","id":"20170320-191608_451907126","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:06:54+0200","dateFinished":"2017-04-01T13:06:54+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9031"},{"title":"Tensorflow example ...","text":"%pyspark\n\nimport tensorflow as tf\nimport math\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\ntf.set_random_seed(0)\n","user":"anonymous","dateUpdated":"2017-04-01T13:07:51+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774273_302521292","id":"20170320-184513_258887949","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:07:51+0200","dateFinished":"2017-04-01T13:07:55+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9032"},{"title":"... example training the MNIST data","text":"%pyspark\n\nmnist = read_data_sets(\"/opt/data/mnist\", one_hot=True, reshape=False, validation_size=0)","user":"anonymous","dateUpdated":"2017-04-01T13:07:57+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774274_303675539","id":"20170320-184533_365540253","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:07:57+0200","dateFinished":"2017-04-01T13:08:11+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9033"},{"title":"Model 1 (5 layer sigmoid)","text":"%pyspark\n\n# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\nX = tf.placeholder(tf.float32, [None, 28, 28, 1])\n\n# correct answers will go here\nY_ = tf.placeholder(tf.float32, [None, 10])\n\n# five layers and their number of neurons (tha last layer has 10 softmax neurons)\nL = 200\nM = 100\nN = 60\nO = 30\n\n# Weights initialised with small random values between -0.2 and +0.2\n# When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\nW1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))  # 784 = 28 * 28\nB1 = tf.Variable(tf.zeros([L]))\nW2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\nB2 = tf.Variable(tf.zeros([M]))\nW3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\nB3 = tf.Variable(tf.zeros([N]))\nW4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\nB4 = tf.Variable(tf.zeros([O]))\nW5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\nB5 = tf.Variable(tf.zeros([10]))\n\n\nXX = tf.reshape(X, [-1, 784])\nY1 = tf.nn.sigmoid(tf.matmul(XX, W1) + B1)\nY2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + B2)\nY3 = tf.nn.sigmoid(tf.matmul(Y2, W3) + B3)\nY4 = tf.nn.sigmoid(tf.matmul(Y3, W4) + B4)\nYlogits = tf.matmul(Y4, W5) + B5\nY = tf.nn.softmax(Ylogits)\n\n\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\ncross_entropy = tf.reduce_mean(cross_entropy)*100\n\n# accuracy of the trained model, between 0 (worst) and 1 (best)\ncorrect_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# All weights for visualization\nallweights = tf.concat([tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1]), tf.reshape(W4, [-1]), tf.reshape(W5, [-1])], 0)\nallbiases  = tf.concat([tf.reshape(B1, [-1]), tf.reshape(B2, [-1]), tf.reshape(B3, [-1]), tf.reshape(B4, [-1]), tf.reshape(B5, [-1])], 0)\n\n# training step, learning rate = 0.003\nlearning_rate = 0.003\ntrain_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n\nmodel = 1\n","user":"anonymous","dateUpdated":"2017-04-01T13:08:14+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"editorHide":false,"title":true,"results":[],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774274_303675539","id":"20170320-184652_1589511078","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:08:14+0200","dateFinished":"2017-04-01T13:08:15+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9034"},{"title":"Model 2 (5 layer relu, lr decay, dropout)","text":"%pyspark\n\nX = tf.placeholder(tf.float32, [None, 28, 28, 1])\n# correct answers will go here\nY_ = tf.placeholder(tf.float32, [None, 10])\n\n# variable learning rate\nlr = tf.placeholder(tf.float32)\n\n# Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\npkeep = tf.placeholder(tf.float32)\n\n# five layers and their number of neurons (tha last layer has 10 softmax neurons)\nL = 200\nM = 100\nN = 60\nO = 30\n# Weights initialised with small random values between -0.2 and +0.2\n# When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\nW1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))  # 784 = 28 * 28\nB1 = tf.Variable(tf.ones([L])/10)\nW2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\nB2 = tf.Variable(tf.ones([M])/10)\nW3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\nB3 = tf.Variable(tf.ones([N])/10)\nW4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\nB4 = tf.Variable(tf.ones([O])/10)\nW5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\nB5 = tf.Variable(tf.zeros([10]))\n\n# The model, with dropout at each layer\nXX = tf.reshape(X, [-1, 28*28])\n\nY1 = tf.nn.relu(tf.matmul(XX, W1) + B1)\nY1d = tf.nn.dropout(Y1, pkeep)\n\nY2 = tf.nn.relu(tf.matmul(Y1d, W2) + B2)\nY2d = tf.nn.dropout(Y2, pkeep)\n\nY3 = tf.nn.relu(tf.matmul(Y2d, W3) + B3)\nY3d = tf.nn.dropout(Y3, pkeep)\n\nY4 = tf.nn.relu(tf.matmul(Y3d, W4) + B4)\nY4d = tf.nn.dropout(Y4, pkeep)\n\nYlogits = tf.matmul(Y4d, W5) + B5\nY = tf.nn.softmax(Ylogits)\n\n# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n# problems with log(0) which is NaN\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\ncross_entropy = tf.reduce_mean(cross_entropy)*100\n\n# accuracy of the trained model, between 0 (worst) and 1 (best)\ncorrect_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nallweights = tf.concat([tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1]), tf.reshape(W4, [-1]), tf.reshape(W5, [-1])], 0)\nallbiases  = tf.concat([tf.reshape(B1, [-1]), tf.reshape(B2, [-1]), tf.reshape(B3, [-1]), tf.reshape(B4, [-1]), tf.reshape(B5, [-1])], 0)\n\n# training step, the learning rate is a placeholder\ntrain_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n\nmodel = 2\n\n\n\n","user":"anonymous","dateUpdated":"2017-04-01T13:13:26+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"editorHide":false,"title":true,"results":[],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774275_303290790","id":"20170320-184721_207897504","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:13:26+0200","dateFinished":"2017-04-01T13:13:27+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9035"},{"title":"Model 3 (5 layer convolutional, relu, lr decay, dropout)","text":"%pyspark\n\n# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\nX = tf.placeholder(tf.float32, [None, 28, 28, 1])\n# correct answers will go here\nY_ = tf.placeholder(tf.float32, [None, 10])\n# variable learning rate\nlr = tf.placeholder(tf.float32)\n# test flag for batch norm\ntst = tf.placeholder(tf.bool)\niter = tf.placeholder(tf.int32)\n# dropout probability\npkeep = tf.placeholder(tf.float32)\npkeep_conv = tf.placeholder(tf.float32)\n\ndef batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) # adding the iteration prevents from averaging across non-existing iterations\n    bnepsilon = 1e-5\n    if convolutional:\n        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n    else:\n        mean, variance = tf.nn.moments(Ylogits, [0])\n    update_moving_everages = exp_moving_avg.apply([mean, variance])\n    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n    return Ybn, update_moving_everages\n\ndef no_batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n    return Ylogits, tf.no_op()\n\ndef compatible_convolutional_noise_shape(Y):\n    noiseshape = tf.shape(Y)\n    noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])\n    return noiseshape\n\n# three convolutional layers with their channel counts, and a\n# fully connected layer (tha last layer has 10 softmax neurons)\nK = 24  # first convolutional layer output depth\nL = 48  # second convolutional layer output depth\nM = 64  # third convolutional layer\nN = 200  # fully connected layer\n\nW1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1))  # 6x6 patch, 1 input channel, K output channels\nB1 = tf.Variable(tf.constant(0.1, tf.float32, [K]))\nW2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\nB2 = tf.Variable(tf.constant(0.1, tf.float32, [L]))\nW3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\nB3 = tf.Variable(tf.constant(0.1, tf.float32, [M]))\n\nW4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\nB4 = tf.Variable(tf.constant(0.1, tf.float32, [N]))\nW5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\nB5 = tf.Variable(tf.constant(0.1, tf.float32, [10]))\n\n# The model\n# batch norm scaling is not useful with relus\n# batch norm offsets are used instead of biases\nstride = 1  # output is 28x28\nY1l = tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME')\nY1bn, update_ema1 = batchnorm(Y1l, tst, iter, B1, convolutional=True)\nY1r = tf.nn.relu(Y1bn)\nY1 = tf.nn.dropout(Y1r, pkeep_conv, compatible_convolutional_noise_shape(Y1r))\nstride = 2  # output is 14x14\nY2l = tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME')\nY2bn, update_ema2 = batchnorm(Y2l, tst, iter, B2, convolutional=True)\nY2r = tf.nn.relu(Y2bn)\nY2 = tf.nn.dropout(Y2r, pkeep_conv, compatible_convolutional_noise_shape(Y2r))\nstride = 2  # output is 7x7\nY3l = tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME')\nY3bn, update_ema3 = batchnorm(Y3l, tst, iter, B3, convolutional=True)\nY3r = tf.nn.relu(Y3bn)\nY3 = tf.nn.dropout(Y3r, pkeep_conv, compatible_convolutional_noise_shape(Y3r))\n\n# reshape the output from the third convolution for the fully connected layer\nYY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n\nY4l = tf.matmul(YY, W4)\nY4bn, update_ema4 = batchnorm(Y4l, tst, iter, B4)\nY4r = tf.nn.relu(Y4bn)\nY4 = tf.nn.dropout(Y4r, pkeep)\nYlogits = tf.matmul(Y4, W5) + B5\nY = tf.nn.softmax(Ylogits)\n\nupdate_ema = tf.group(update_ema1, update_ema2, update_ema3, update_ema4)\n\n# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n# problems with log(0) which is NaN\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\ncross_entropy = tf.reduce_mean(cross_entropy)*100\n\n# accuracy of the trained model, between 0 (worst) and 1 (best)\ncorrect_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# matplotlib visualisation\nallweights = tf.concat([tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1]), tf.reshape(W4, [-1]), tf.reshape(W5, [-1])], 0)\nallbiases  = tf.concat([tf.reshape(B1, [-1]), tf.reshape(B2, [-1]), tf.reshape(B3, [-1]), tf.reshape(B4, [-1]), tf.reshape(B5, [-1])], 0)\nconv_activations = tf.concat([tf.reshape(tf.reduce_max(Y1r, [0]), [-1]), tf.reshape(tf.reduce_max(Y2r, [0]), [-1]), tf.reshape(tf.reduce_max(Y3r, [0]), [-1])], 0)\ndense_activations = tf.reduce_max(Y4r, [0])\n\n\n# training step, the learning rate is a placeholder\ntrain_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n\nmodel = 3\n","user":"anonymous","dateUpdated":"2017-04-01T13:12:22+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774275_303290790","id":"20170321-095544_960868384","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:12:22+0200","dateFinished":"2017-04-01T13:12:23+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9036"},{"title":"The training function","text":"%pyspark\ndef train(train_period, batch_size):\n    for i in range(training_steps+1):\n\n        # training on batches of 100 images with 100 labels\n        batch_X, batch_Y = mnist.train.next_batch(100)\n\n        if model == 2:\n            # learning rate decay\n            max_learning_rate = 0.003\n            min_learning_rate = 0.0001\n            decay_speed = 2000.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n        elif model == 3:\n            max_learning_rate = 0.02\n            min_learning_rate = 0.0001\n            decay_speed = 1600\n            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n\n        # compute test values for visualisation\n        if i % batch_size == 0:\n            if model == 1:\n                a_test, l_test = sess.run([accuracy, cross_entropy], {X: mnist.test.images, Y_: mnist.test.labels})\n            elif model == 2:\n                a_test, l_test = sess.run([accuracy, cross_entropy], {X: mnist.test.images, Y_: mnist.test.labels, pkeep:1.0})\n            elif model == 3:\n                a_test, l_test = sess.run([accuracy, cross_entropy], {X: mnist.test.images, Y_: mnist.test.labels, pkeep: 1.0, pkeep_conv: 1.0, tst: True})\n\n        # compute training values for visualisation\n        if i % train_period == 0:\n            if model == 1:\n                a_train, l_train, w, b = sess.run([accuracy, cross_entropy, allweights, allbiases], {X: batch_X, Y_: batch_Y})\n            elif model == 2:\n                a_train, l_train, w, b = sess.run([accuracy, cross_entropy, allweights, allbiases], {X: batch_X, Y_: batch_Y, pkeep:1.0})\n            elif model == 3:\n                a_train, l_train, w, b = sess.run([accuracy, cross_entropy, allweights, allbiases], {X: batch_X, Y_: batch_Y, pkeep: 1.0, pkeep_conv: 1.0, tst: False})\n\n            alAppend(i, a_train, a_test, l_train, l_test)\n\n        if breaker.isStopped():\n            print(\"Interrupted\")\n            break\n        \n        # the backpropagation training step\n        if model == 1:  \n            sess.run(train_step, {X: batch_X, Y_: batch_Y})\n        elif model == 2:\n            sess.run(train_step, {X: batch_X, Y_: batch_Y, pkeep: 0.75, lr: learning_rate})\n        elif model == 3:\n            sess.run(train_step, {X: batch_X, Y_: batch_Y, lr: learning_rate, tst: False, pkeep: 0.75, pkeep_conv: 1.0})\n            sess.run(update_ema, {X: batch_X, Y_: batch_Y, tst: False, iter: i, pkeep: 1.0, pkeep_conv: 1.0})\n","user":"anonymous","dateUpdated":"2017-04-01T13:10:55+0200","config":{"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774276_301367045","id":"20170329-151831_1515719707","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:10:55+0200","dateFinished":"2017-04-01T13:10:55+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9037"},{"title":"Step 1: Initialise Model","text":"%pyspark\n\n# init\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\nif model in [1,2]:\n    training_steps = 10000\nelif model == 3:\n    training_steps = 5000\n\n","user":"anonymous","dateUpdated":"2017-04-01T13:13:34+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774276_301367045","id":"20170320-184935_1115960941","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:13:34+0200","dateFinished":"2017-04-01T13:13:34+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9038"},{"title":"Step 2: Prepare Visualisations","text":"%pyspark\n\n\n# define plot configuration\nlmax = 20  # max loss to be plotted\n\nconfig={\"height\":500, \"width\": 500, \"color\":nv.c10(1,0), \"duration\":0,\n        \"xDomain\":[0,training_steps],\"xAxis\":{\"axisLabel\":\"Step\", \"tickFormat\":\",d\"} }\n\naConfig = dict(yDomain=[0.95,1], yAxis={\"axisLabel\":\"Accuracy\", \"tickFormat\":\",.3f\"}, **config)\nlConfig = dict(yDomain=[0,lmax], yAxis={\"axisLabel\":\"Loss\",     \"tickFormat\":\",.3f\"}, **config)\n\n\n# Create data in the correct format\ndef data(typ, x, y_test, y_train):\n    return {\"step\":[x], \"train_%s\"%typ:[y_train], \"test_%s\"%typ:[y_test]}\n\nal = nv.lineChart()\n\n# create a horizontal plot with two charts\nal.hplot([al.chart(data(\"accuracy\", 0, 0, 0),   \"step\", [\"test_accuracy\", \"train_accuracy\"], \n                   config=aConfig),\n          al.chart(data(\"loss\", 0, lmax, lmax), \"step\", [\"test_loss\",     \"train_loss\"    ], \n                   config=lConfig)])\n\n# append data to plots\ndef alAppend(i, a_train, a_test, l_train, l_test):\n    al.append(data(\"accuracy\", i, a_train, a_test), chart=0)\n    al.append(data(\"loss\",     i, l_train, l_test), chart=1)\n\n\n","user":"anonymous","dateUpdated":"2017-04-01T13:13:37+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774277_300982296","id":"20170320-190740_184779135","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:13:37+0200","dateFinished":"2017-04-01T13:13:37+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9039"},{"title":"Step 3: Start Learning","text":"%pyspark\n\nbreaker.start()\n\ntrain_period = 20\nbatch_size = 5 * train_period\n\ntrain(train_period, batch_size)\n","user":"anonymous","dateUpdated":"2017-04-01T13:13:43+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774278_302136543","id":"20170320-184956_312642282","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:13:43+0200","dateFinished":"2017-04-01T13:14:34+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9040"},{"title":"The interrupt button","text":"%pyspark\n\nbreaker = Breaker(z.z)\n","user":"anonymous","dateUpdated":"2017-04-01T13:11:28+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774278_302136543","id":"20170320-211913_877016685","dateCreated":"2017-04-01T13:06:14+0200","dateStarted":"2017-04-01T13:11:28+0200","dateFinished":"2017-04-01T13:11:28+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9041"},{"title":"Save charts","text":"%pyspark\nimport time\nalChart.saveAsPng(\"accuracy%d.png\" % model, chart=0)\ntime.sleep(3)\nalChart.saveAsPng(\"loss%d.png\" % model, chart=1)\n","dateUpdated":"2017-04-01T13:06:14+0200","config":{"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774277_300982296","id":"20170320-215911_2018209483","dateCreated":"2017-04-01T13:06:14+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9042"},{"dateUpdated":"2017-04-01T13:06:14+0200","config":{"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1491044774279_301751794","id":"20170320-185044_1983535247","dateCreated":"2017-04-01T13:06:14+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9043"}],"name":"Information is Beautiful (4 of 3)","id":"2CDNXTU8X","angularObjects":{"2C8CW5MV1:shared_process":[{"name":"__stop__","object":1,"noteId":"2CDNXTU8X"},{"name":"__zeppelin_comm_2CDNXTU8X_status__","object":"ZeppelinSession started (do not delete this paragraph)","noteId":"2CDNXTU8X"},{"name":"__zeppelin_comm_2CDNXTU8X_msg__","object":{"msg":{"delay":0,"function":"lineChart","object":{"data":[{"values":[{"x":7900,"y":5.234006404876709}],"key":"test_loss"},{"values":[{"x":7900,"y":8.100438117980957}],"key":"train_loss"}],"plotId":"lineChart-036","event":"append"}},"task":"call","id":1380},"noteId":"2CDNXTU8X"}],"2CBWP5YFP:shared_process":[],"2CBEDQHXY:shared_process":[],"2C97UNR7H:shared_process":[],"2C9CWX9EW:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}